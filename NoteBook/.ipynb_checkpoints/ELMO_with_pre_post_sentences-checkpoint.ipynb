{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8i0HWQbRjhyA"
   },
   "source": [
    "# Pre/Post Sentences\n",
    "* Still using ELMO as embedding layer\n",
    "* Include pre/post sentences of the target sentences\n",
    "* Perhaps the model can learn the sequential relationship between them\n",
    "* The code will be pretty similar to the **ELMO.ipynb**\n",
    "* Since during the time I wrote this notebook, there's only ELMO pretrained model implemented in *TF1.x*, so we return to using TensorFlow 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Ie16w2Iei5IE",
    "outputId": "c5a11f65-dd13-44f6-941f-19cdd49efb6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
      "You set: `1.x  # use TF1 here`. This will be interpreted as: `1.x`.\n",
      "\n",
      "\n",
      "TensorFlow 1.x selected.\n",
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x  # use TF1 here\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.engine import Layer\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__) # confirm version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYD_HY-o6jFw"
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHqfDGq1lbJD"
   },
   "source": [
    "## Load the data\n",
    "* No stemming\n",
    "* Here upload another file that only contain the label (just for upload simplicity)\n",
    "* No need to pad each sentence to same length \n",
    "* No need to Tokenize\n",
    "\n",
    "**ELMO Module will do those tasks for us !!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0V1dtPflXog"
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "BATCH_SIZE = 64\n",
    "EMBED_SIZE = 1024\n",
    "MAX_LENGTH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96EQnc0ll2h"
   },
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('train_prepost.csv')\n",
    "LABEL = pd.read_csv('train_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mKQ5dbpxlpZ5",
    "outputId": "ec480149-e677-463e-aa0a-56151384dd2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train.shape:  (35150, 8)\n",
      "DATA_val.shape:  (11717, 8)\n",
      "LABEL_train.shape:  (35150, 6)\n",
      "LABEL_val.shape:  (11717, 6)\n"
     ]
    }
   ],
   "source": [
    "#split to train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_train, DATA_val, LABEL_train, LABEL_val = train_test_split(DATA,LABEL, test_size=0.25, random_state=0)\n",
    "print('DATA_train.shape: ', DATA_train.shape)\n",
    "print('DATA_val.shape: ', DATA_val.shape)\n",
    "print('LABEL_train.shape: ', LABEL_train.shape)\n",
    "print('LABEL_val.shape: ', LABEL_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ejvvN7wYlq-g",
    "outputId": "05b46def-3650-4085-9049-9002d4be4449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35150, 1) (11717, 1)\n",
      "(35150, 1) (11717, 1)\n",
      "(35150, 1) (11717, 1)\n",
      "(35150, 2) (11717, 2)\n",
      "(35150, 6) (11717, 6)\n"
     ]
    }
   ],
   "source": [
    "Sen_train = DATA_train.loc[:,'Sentences']\n",
    "# Sen_train = [' '.join(t.split()[0:MAX_LENGTH]) for t in Sen_train]  # restrict the maximum length of the sentences (if you want, it might accelerate the training/testing time)\n",
    "Sen_train = np.array(Sen_train, dtype=object)[:, np.newaxis]\n",
    "PreSen_train = DATA_train.loc[:,'PRE_Sen']\n",
    "PreSen_train = np.array(PreSen_train, dtype=object)[:, np.newaxis]\n",
    "PostSen_train = DATA_train.loc[:,'POST_Sen']\n",
    "PostSen_train = np.array(PostSen_train, dtype=object)[:, np.newaxis]\n",
    "\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# PrePreSen_train = DATA_train.loc[:,'PREPRE_Sen']\n",
    "# PrePreSen_train = np.array(PrePreSen_train, dtype=object)[:, np.newaxis]\n",
    "# PostPostSen_train = DATA_train.loc[:,'POSTPOST_Sen']\n",
    "# PostPostSen_train = np.array(PostPostSen_train, dtype=object)[:, np.newaxis]\n",
    "#===============================================================================\n",
    "\n",
    "POSITION_train = DATA_train.loc[:,'POSITION':'TOTAL_LEN'].to_numpy()\n",
    "LABEL_train = LABEL_train.to_numpy()\n",
    "\n",
    "\n",
    "Sen_val = DATA_val.loc[:,'Sentences']\n",
    "# Sen_val = [' '.join(t.split()[0:MAX_LENGTH]) for t in Sen_val]  # restrict the maximum length of the sentences (if you need)\n",
    "Sen_val = np.array(Sen_val, dtype=object)[:, np.newaxis]\n",
    "PreSen_val = DATA_val.loc[:,'PRE_Sen']\n",
    "PreSen_val = np.array(PreSen_val, dtype=object)[:, np.newaxis]\n",
    "PostSen_val = DATA_val.loc[:,'POST_Sen']\n",
    "PostSen_val = np.array(PostSen_val, dtype=object)[:, np.newaxis]\n",
    "\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# PrePreSen_val = DATA_val.loc[:,'PREPRE_Sen']\n",
    "# PrePreSen_val = np.array(PrePreSen_val, dtype=object)[:, np.newaxis]\n",
    "# PostPostSen_val = DATA_val.loc[:,'POSTPOST_Sen']\n",
    "# PostPostSen_val = np.array(PostPostSen_val, dtype=object)[:, np.newaxis]\n",
    "#===============================================================================\n",
    "\n",
    "POSITION_val = DATA_val.loc[:,'POSITION':'TOTAL_LEN'].to_numpy()\n",
    "LABEL_val = LABEL_val.to_numpy()\n",
    "\n",
    "print(Sen_train.shape, Sen_val.shape)\n",
    "print(PreSen_train.shape, PreSen_val.shape)\n",
    "print(PostSen_train.shape, PostSen_val.shape)\n",
    "print(POSITION_train.shape, POSITION_val.shape)\n",
    "print(LABEL_train.shape, LABEL_val.shape)\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# print(PrePreSen_train.shape, PrePreSen_val.shape)\n",
    "# print(PostPostSen_train.shape, PostPostSen_val.shape)\n",
    "#==============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pChZOY5nuea"
   },
   "source": [
    "## Define the model\n",
    "Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
    "* Three ELMO, for Target Sentences, Pre Sentences and Post Sentences\n",
    "* Maybe change to only one ELMO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAAGF1FMu7T5"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHoFUnrQoJ6f"
   },
   "outputs": [],
   "source": [
    "# Function to calculate F1_score\n",
    "def F1_score(y_true, y_pred):\n",
    "  DTYPE = tf.float32\n",
    "  THRESHOLD = 0.5\n",
    "\n",
    "  y_pred = tf.cast(y_pred > THRESHOLD, DTYPE) \n",
    "\n",
    "  true_positives = tf.math.count_nonzero(tf.math.logical_and(tf.math.equal(y_pred,1.0), tf.math.equal(y_true,1.0)), axis=0)\n",
    "  false_positives = tf.math.count_nonzero(tf.math.logical_and(tf.math.equal(y_pred,1.0), tf.math.equal(y_true,0.0)), axis=0)\n",
    "  false_negatives = tf.math.count_nonzero(tf.math.logical_and(tf.math.equal(y_pred,0.0), tf.math.equal(y_true,1.0)), axis=0)\n",
    "\n",
    "  TP = tf.math.reduce_sum(tf.cast(true_positives, DTYPE), axis=0)\n",
    "  FP = tf.math.reduce_sum(tf.cast(false_positives, DTYPE), axis=0)\n",
    "  FN = tf.math.reduce_sum(tf.cast(false_negatives, DTYPE), axis=0)\n",
    "\n",
    "  precision = tf.math.divide_no_nan(TP, TP+FP)\n",
    "  recall = tf.math.divide_no_nan(TP, TP+FN)\n",
    "\n",
    "  F1 = tf.math.divide_no_nan(2 * (precision * recall) , (precision + recall))\n",
    "  return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_nYTkJL0KM-"
   },
   "source": [
    "### ELMO as word embedding\n",
    "* Using pretrained ELMO as word embedding layer\n",
    "* The weight for combining the output of each layer of the ELMO is trainable(Only 4 variables) [REF](https://tfhub.dev/google/elmo/3)\n",
    "* [Source](https://github.com/strongio/keras-elmo/blob/master/Elmo%20Keras.ipynb)\n",
    "* Return size =  ``` [Batch_size(None), max_length(None), 1024] ```\n",
    "\n",
    "(The ELMO module will automatcally detect the max_length of each input batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVuvBGeMn_Yf"
   },
   "outputs": [],
   "source": [
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      self.dimensions = 1024\n",
    "      self.trainable=True\n",
    "      super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      self.elmo = hub.Module('https://tfhub.dev/google/elmo/3', trainable=self.trainable,\n",
    "                              name=\"{}_module\".format(self.name))\n",
    "\n",
    "      self.trainable_weights += tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "      super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "      result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                    as_dict=True,\n",
    "                    signature='default',\n",
    "                    )['elmo']\n",
    "      return result\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return (input_shape[0], None, self.dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PekgB23D1I3j"
   },
   "source": [
    "### ELMO as sentences embedding\n",
    "* Same as above, but the return is **default**. The model will return the mean-pooling of all words embedding result\n",
    "* Return size =  ``` [Batch_size(None), 1024] ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kD4ySmwJ-N6j"
   },
   "outputs": [],
   "source": [
    "class ElmoSentenceEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      self.dimensions = 1024\n",
    "      self.trainable=True\n",
    "      super(ElmoSentenceEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "      self.elmo = hub.Module('https://tfhub.dev/google/elmo/3', trainable=self.trainable,\n",
    "                              name=\"{}_module\".format(self.name))\n",
    "\n",
    "      self.trainable_weights += tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "      super(ElmoSentenceEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "      result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                    as_dict=True,\n",
    "                    signature='default',\n",
    "                    )['default']\n",
    "      return result\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "      return (input_shape[0], self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IVFy8Hlh-6F"
   },
   "outputs": [],
   "source": [
    "def concatresult(inputs):\n",
    "  inputs_1_expand = K.expand_dims(inputs[0], 1)\n",
    "  inputs_2_expand = K.expand_dims(inputs[1], 1)\n",
    "  inputs_3_expand = K.expand_dims(inputs[2], 1)\n",
    "  result = K.concatenate([inputs_1_expand,inputs_2_expand,inputs_3_expand], axis=1)\n",
    "  \n",
    "  #%% see blocks below - 2 Pre/Post Sentences\n",
    "  #=============================================================================\n",
    "  # inputs_4_expand = K.expand_dims(inputs[3], 1)\n",
    "  # inputs_5_expand = K.expand_dims(inputs[4], 1)\n",
    "  # result = K.concatenate([inputs_1_expand,inputs_2_expand,inputs_3_expand,inputs_4_expand,inputs_5_expand], axis=1)\n",
    "  #=============================================================================\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpelAmc63UOK"
   },
   "outputs": [],
   "source": [
    "# use to concat the position in the input of the word vector, do not use in the final version of the modle\n",
    "def repeat_and_concatenate(inputs):\n",
    "    embed, position = inputs\n",
    "    # Repeat 2D vectors\n",
    "    position_repeat = K.tile(K.expand_dims(position, 1), [1, K.shape(embed)[1], 1])\n",
    "    # Concatenate feature-wise\n",
    "    return K.concatenate([embed, position_repeat], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4_tBgpIuSZz"
   },
   "source": [
    "### Model_1\n",
    "* Using three ELMO as word embedding, then input to Bi_GRU as sentences embedding\n",
    "* Later, using a Dense layer to reduce the length of **Pre_Sentence** and **Post_Sentence**\n",
    "* Concat all vectors (include some features other than text input) as feature vector of the target sentence, then using a Dense layer as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yccJyGp_oOOW"
   },
   "outputs": [],
   "source": [
    "# Function to build model\n",
    "def build_model(): \n",
    "  input_text = layers.Input(shape=(1,), name='Target_sentences', dtype=tf.string)\n",
    "  input_pre = layers.Input(shape=(1,), name='Pre_sentences', dtype=tf.string)\n",
    "  input_post = layers.Input(shape=(1,), name='Post_sentences', dtype=tf.string)\n",
    "  input_position = layers.Input(shape=(2,), name='Sentence_position', dtype=tf.float32)\n",
    "\n",
    "  embedding_target = ElmoEmbeddingLayer(name=\"ElmoEmbed_target\")(input_text)\n",
    "  # test_position = layers.Lambda(repeat_and_concatenate, name='Merge_position')([embedding_target, input_position])  #version2: add position in the embedding of the words\n",
    "  GRU_target = layers.Bidirectional(layers.GRU(256, dropout=0.5, return_sequences=False), name='BiGRU_target')(embedding_target)\n",
    "\n",
    "  embedding_pre = ElmoEmbeddingLayer(name=\"ElmoEmbed_pre\")(input_pre)\n",
    "  GRU_pre = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_pre')(embedding_pre)\n",
    "  embedding_post = ElmoEmbeddingLayer(name=\"ElmoEmbed_post\")(input_post)\n",
    "  GRU_post = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_post')(embedding_post)\n",
    "  \n",
    "  pre = layers.Dense(20, activation='sigmoid')(GRU_pre)\n",
    "  post = layers.Dense(20, activation='sigmoid')(GRU_post)\n",
    "\n",
    "  concat = layers.concatenate([pre, GRU_target, post, input_position], name='Merge')\n",
    "\n",
    "  pred = layers.Dense(6, activation='sigmoid')(concat)\n",
    "\n",
    "  model = Model(inputs=[input_text,input_pre,input_post,input_position], outputs=pred)\n",
    "  adam = Adam(lr=0.001)\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[F1_score])\n",
    "  model.summary()\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "# save the weight for later retraining the whole module\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCt-scZ5uVry"
   },
   "source": [
    "### Model_2\n",
    "* Using Dense layer as kind of summary of the output of ELMOSentenceEmbedding\n",
    "* Later, using another Bi_GRU as an encoder to encode the \\[Pre_Sentence, Target_Sentence, Post_Sentence\\]\n",
    "* The output of the encoder is the summary of these three sentences which bias to the target sentence\n",
    "* Concatenate with some non-text input to form the final feature vector of the target sentence, then using a Dense layer as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "rsXZWekvGGWc",
    "outputId": "32be3d28-0ae2-4507-b1d4-160421f76bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Pre_sentences (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Target_sentences (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Post_sentences (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ElmoEmbed_pre (ElmoSentenceEmbe (None, 1024)         4           Pre_sentences[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ElmoEmbed_target (ElmoEmbedding (None, None, 1024)   4           Target_sentences[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ElmoEmbed_post (ElmoSentenceEmb (None, 1024)         4           Post_sentences[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          262400      ElmoEmbed_pre[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "BiGRU_target (Bidirectional)    (None, 256)          885504      ElmoEmbed_target[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          262400      ElmoEmbed_post[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Merge_Sentences (Lambda)        (None, 3, 256)       0           dense_1[0][0]                    \n",
      "                                                                 BiGRU_target[0][0]               \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BiGRU_Summary (Bidirectional)   (None, 256)          295680      Merge_Sentences[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Sentence_position (InputLayer)  (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Merge_position (Concatenate)    (None, 258)          0           BiGRU_Summary[0][0]              \n",
      "                                                                 Sentence_position[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            1554        Merge_position[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,707,550\n",
      "Trainable params: 1,707,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Function to build model\n",
    "def build_model(): \n",
    "  input_text = layers.Input(shape=(1,), name='Target_sentences', dtype=tf.string)\n",
    "  input_pre = layers.Input(shape=(1,), name='Pre_sentences', dtype=tf.string)\n",
    "  input_post = layers.Input(shape=(1,), name='Post_sentences', dtype=tf.string)\n",
    "  input_position = layers.Input(shape=(2,), name='Sentence_position', dtype=tf.float32)\n",
    "\n",
    "  embedding_target = ElmoEmbeddingLayer(name=\"ElmoEmbed_target\")(input_text)\n",
    "  # target_withposition = layers.Lambda(repeat_and_concatenate, name='Merge_position')([embedding_target, input_position]) #version2: add position in the embedding of the words\n",
    "  GRU_target = layers.Bidirectional(layers.GRU(128, dropout=0.5, return_sequences=False), name='BiGRU_target')(embedding_target)\n",
    "\n",
    "  embedding_pre = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_pre\")(input_pre)\n",
    "  GRU_pre = layers.Dense(256, activation='sigmoid')(embedding_pre)\n",
    "  embedding_post = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_post\")(input_post)\n",
    "  GRU_post = layers.Dense(256, activation='sigmoid')(embedding_post)\n",
    "  \n",
    "  #%% we can also encode the sentences same with Model_1, but since this will result in lots of layers of Bi_GRU which will cost too much time to train\n",
    "  # embedding_pre = ElmoEmbeddingLayer(name=\"ElmoEmbed_pre\")(input_pre)\n",
    "  # GRU_pre = layers.Bidirectional(layers.GRU(256, dropout=0.5, return_sequences=False), name='BiGRU_pre')(embedding_pre)\n",
    "  # embedding_post = ElmoEmbeddingLayer(name=\"ElmoEmbed_post\")(input_post)\n",
    "  # GRU_post = layers.Bidirectional(layers.GRU(256, dropout=0.5, return_sequences=False), name='BiGRU_post')(embedding_post)\n",
    "\n",
    "  concat =  layers.Lambda(concatresult, name='Merge_Sentences')([GRU_pre, GRU_target, GRU_post])\n",
    "  Summary = layers.Bidirectional(layers.GRU(128, dropout=0.5, return_sequences=False), name='BiGRU_Summary')(concat)\n",
    "\n",
    "  addposition = layers.concatenate([Summary, input_position], name='Merge_position')\n",
    "  pred = layers.Dense(6, activation='sigmoid')(addposition)\n",
    "\n",
    "  model = Model(inputs=[input_text,input_pre,input_post,input_position], outputs=pred)\n",
    "  adam = Adam(lr=0.001)\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[F1_score])\n",
    "  model.summary()\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "# save the weight for later retraining the whole module\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27pcqzYfqmDH"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieB25-CbqlK4"
   },
   "outputs": [],
   "source": [
    "# Monitor the performance\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_F1_score', patience=3, mode='max', restore_best_weights=True)\n",
    "\n",
    "history = model.fit([Sen_train,PreSen_train,PostSen_train,POSITION_train], LABEL_train,\n",
    "          validation_data=([Sen_val,PreSen_val,PostSen_val,POSITION_val], LABEL_val),\n",
    "          epochs=20, batch_size=BATCH_SIZE,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFjX9fkhrJZ9"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "TVMngouGrJwS",
    "outputId": "9eb5ad0b-d473-4df8-d5ec-817c2d0873cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11717, 6)\n",
      "[[0.00757217 0.06803593 0.9603202  0.03049085 0.00372973 0.00290501]\n",
      " [0.00969911 0.5053471  0.1252133  0.18213439 0.39775497 0.04539654]\n",
      " [0.9133489  0.10974118 0.01804778 0.0061903  0.00242183 0.01721632]\n",
      " [0.00727654 0.57898104 0.471901   0.07808304 0.01865387 0.00378391]]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict([Sen_val,PreSen_val,PostSen_val,POSITION_val,PrePreSen_val,PostPostSen_val])\n",
    "\n",
    "print(result.shape)\n",
    "print(result[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jLnA_FHJrL44",
    "outputId": "aec78293-88a2-49fe-f9aa-fd589414f8b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11717, 6)\n",
      "(11717, 6)\n",
      "0.6935446282917734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "greater = (result>=0.5).astype(int)\n",
    "print(LABEL_val.shape)\n",
    "print(greater.shape)\n",
    "print(f1_score(LABEL_val, greater, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbIZ8RqIbhvW"
   },
   "source": [
    "Examine the validation result\n",
    "* Also store the Sentence so that we can examine under which condition of the sentences our prediction will go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w55h_GJSbk-q"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(result)\n",
    "df2 = pd.DataFrame(LABEL_val)\n",
    "df3 = pd.DataFrame(Sen_val) # store the sentences too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CjOCf7sXcFLp",
    "outputId": "d1556f59-be85-4fe6-e3c3-e6f6cb4c5b82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009559</td>\n",
       "      <td>0.146008</td>\n",
       "      <td>0.229192</td>\n",
       "      <td>0.745078</td>\n",
       "      <td>0.305471</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>We demonstrate some of the useful properties a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.866730</td>\n",
       "      <td>0.101646</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As the technologies used within NAME advance i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647780</td>\n",
       "      <td>0.408719</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yet these methods often fail to capture the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.724096</td>\n",
       "      <td>0.466831</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.010023</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Generating novel yet realistic images of perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011359</td>\n",
       "      <td>0.023873</td>\n",
       "      <td>0.229095</td>\n",
       "      <td>0.829455</td>\n",
       "      <td>0.050550</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Evaluations using separate genre classifiers s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  ...  5                                                  0\n",
       "0  0.009559  0.146008  ...  0  We demonstrate some of the useful properties a...\n",
       "1  0.866730  0.101646  ...  0  As the technologies used within NAME advance i...\n",
       "2  0.647780  0.408719  ...  0  Yet these methods often fail to capture the mo...\n",
       "3  0.724096  0.466831  ...  0  Generating novel yet realistic images of perso...\n",
       "4  0.011359  0.023873  ...  0  Evaluations using separate genre classifiers s...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result = pd.concat([df1,df2, df3], axis=1)\n",
    "val_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gekSjSpMc8oO"
   },
   "outputs": [],
   "source": [
    "val_result.to_csv(\"val_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNrcXKVArNlD"
   },
   "source": [
    "### Refit on the whole data with around 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goAZDOHzrN5U"
   },
   "outputs": [],
   "source": [
    "Sen = DATA.loc[:,'Sentences']\n",
    "# Sen = [' '.join(t.split()[0:MAX_LENGTH]) for t in Sen]  # restrict the maximum length of the sentences (if you need)\n",
    "Sen = np.array(Sen, dtype=object)[:, np.newaxis]\n",
    "PreSen = DATA.loc[:,'PRE_Sen']\n",
    "PreSen = np.array(PreSen, dtype=object)[:, np.newaxis]\n",
    "PostSen = DATA.loc[:,'POST_Sen']\n",
    "PostSen = np.array(PostSen, dtype=object)[:, np.newaxis]\n",
    "\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# PrePreSen = DATA.loc[:,'PREPRE_Sen']\n",
    "# PrePreSen = np.array(PrePreSen, dtype=object)[:, np.newaxis]\n",
    "# PostPostSen = DATA.loc[:,'POSTPOST_Sen']\n",
    "# PostPostSen = np.array(PostPostSen, dtype=object)[:, np.newaxis]\n",
    "#===============================================================================\n",
    "\n",
    "POSITION = DATA.loc[:,'POSITION':'TOTAL_LEN'].to_numpy()\n",
    "LABEL = LABEL.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "lIqNRWILrf45",
    "outputId": "83f0ac9f-6ea5-4c95-ade1-51b632ad82b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46867, 1)\n",
      "(46867, 1)\n",
      "(46867, 1)\n",
      "(46867, 2)\n",
      "(46867, 6)\n"
     ]
    }
   ],
   "source": [
    "print(Sen.shape)\n",
    "print(PreSen.shape)\n",
    "print(PostSen.shape)\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# print(PrePreSen.shape)\n",
    "# print(PostPostSen.shape)\n",
    "#===============================================================================\n",
    "print(POSITION.shape)\n",
    "print(LABEL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2d4Ous7-rnIE",
    "outputId": "a3a240a5-dfcf-4314-d630-4e11aadb376b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "46867/46867 [==============================] - 793s 17ms/step - loss: 0.3094 - F1_score: 0.5993\n",
      "Epoch 2/5\n",
      "46867/46867 [==============================] - 796s 17ms/step - loss: 0.2764 - F1_score: 0.6675\n",
      "Epoch 3/5\n",
      "46867/46867 [==============================] - 803s 17ms/step - loss: 0.2660 - F1_score: 0.6833\n",
      "Epoch 4/5\n",
      "46867/46867 [==============================] - 771s 16ms/step - loss: 0.2574 - F1_score: 0.6962\n",
      "Epoch 5/5\n",
      "46867/46867 [==============================] - 766s 16ms/step - loss: 0.2502 - F1_score: 0.7057\n"
     ]
    }
   ],
   "source": [
    "# reload the model weight\n",
    "model.set_weights(weights)\n",
    "# train with best epoch = 5\n",
    "history = model.fit([Sen,PreSen,PostSen,POSITION], LABEL, epochs=5, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VIPXIkorzTl"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9LHomEVdddlR"
   },
   "outputs": [],
   "source": [
    "TEST_DATA = pd.read_csv('test_prepost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPaj-Oq4dh2e"
   },
   "outputs": [],
   "source": [
    "Test_Sen = TEST_DATA.loc[:,'Sentences']\n",
    "# Test_Sen = [' '.join(t.split()[0:MAX_LENGTH]) for t in Test_Sen]  # restrict the maximum length of the sentences (if you need)\n",
    "Test_Sen = np.array(Test_Sen, dtype=object)[:, np.newaxis]\n",
    "Test_PreSen = TEST_DATA.loc[:,'PRE_Sen']\n",
    "Test_PreSen = np.array(Test_PreSen, dtype=object)[:, np.newaxis]\n",
    "Test_PostSen = TEST_DATA.loc[:,'POST_Sen']\n",
    "Test_PostSen = np.array(Test_PostSen, dtype=object)[:, np.newaxis]\n",
    "\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# Test_PrePreSen = TEST_DATA.loc[:,'PREPRE_Sen']\n",
    "# Test_PrePreSen = np.array(Test_PrePreSen, dtype=object)[:, np.newaxis]\n",
    "# Test_PostPostSen = TEST_DATA.loc[:,'POSTPOST_Sen']\n",
    "# Test_PostPostSen = np.array(Test_PostPostSen, dtype=object)[:, np.newaxis]\n",
    "#===============================================================================\n",
    "\n",
    "Test_POSITION = TEST_DATA.loc[:,'POSITION':'TOTAL_LEN'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2Md4mlEidvzW",
    "outputId": "4a54792e-b5ed-4967-ee2e-2f04f7856bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131782, 1)\n",
      "(131782, 1)\n",
      "(131782, 1)\n",
      "(131782, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Test_Sen.shape)\n",
    "print(Test_PreSen.shape)\n",
    "print(Test_PostSen.shape)\n",
    "#%% see blocks below - 2 Pre/Post Sentences\n",
    "#===============================================================================\n",
    "# print(Test_PrePreSen.shape)\n",
    "# print(Test_PostPostSen.shape)\n",
    "#===============================================================================\n",
    "print(Test_POSITION.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6wosCHPqdycY",
    "outputId": "f42d00f7-958d-4c98-b0ff-c3d527b99b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131782, 6)\n",
      "[[0.859565   0.05170634 0.00781983 0.00469366 0.00302225 0.00627455]\n",
      " [0.92724466 0.01253507 0.00733677 0.01177165 0.00392583 0.00221708]\n",
      " [0.5364166  0.11997902 0.00786909 0.00517094 0.00275862 0.00648162]\n",
      " [0.00192121 0.8123803  0.408198   0.07168016 0.04696891 0.00132227]]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict([Test_Sen,Test_PreSen,Test_PostSen,Test_POSITION])\n",
    "\n",
    "print(result.shape)\n",
    "print(result[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "zObhprg9d49f",
    "outputId": "2fcecac8-a902-44ee-8e21-3c00b10f7dc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993274</td>\n",
       "      <td>0.014937</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.003719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.870303</td>\n",
       "      <td>0.256270</td>\n",
       "      <td>0.020141</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.010243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.948224</td>\n",
       "      <td>0.120969</td>\n",
       "      <td>0.030201</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.005209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.446890</td>\n",
       "      <td>0.832253</td>\n",
       "      <td>0.029567</td>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.002820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.051138</td>\n",
       "      <td>0.955495</td>\n",
       "      <td>0.056679</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.003050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BACKGROUND  OBJECTIVES   METHODS   RESULTS  CONCLUSIONS    OTHERS\n",
       "0    0.993274    0.014937  0.002228  0.001413     0.000702  0.003719\n",
       "1    0.870303    0.256270  0.020141  0.005964     0.002896  0.010243\n",
       "2    0.948224    0.120969  0.030201  0.003545     0.002351  0.005209\n",
       "3    0.007879    0.446890  0.832253  0.029567     0.014735  0.002820\n",
       "4    0.005733    0.051138  0.955495  0.056679     0.009260  0.003050"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT = pd.DataFrame(result, columns=['BACKGROUND','OBJECTIVES','METHODS','RESULTS','CONCLUSIONS','OTHERS'])\n",
    "RESULT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgSxaPMZtVBf"
   },
   "outputs": [],
   "source": [
    "RESULT.to_csv(\"test_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0j54vXF4Mzu"
   },
   "outputs": [],
   "source": [
    "# save the weight\n",
    "# model.save_weights('ELMO_withprepostsentences.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atCaR9Ebso13"
   },
   "source": [
    "## Load weight\n",
    "If we got new input testing data, we can initialize the model first then load the weight we have saved to do the prediction\n",
    "* need to initialize the model beforehand since we only save the weight but not the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuP0_Okxs41i"
   },
   "outputs": [],
   "source": [
    "# model = build_model()\n",
    "# model.load_weights('ELMO_withprepostsentences.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvL9XRx9wIbH"
   },
   "source": [
    "# 2 Pre/Post Sentences\n",
    "* Same as above, but add more pre/post sentences of the target sentences\n",
    "* Perhaps the model can learn more from more context of the target sentences\n",
    "* you can find some commented line in the above with the \n",
    "  - prepre\n",
    "  - postpost \n",
    "\n",
    "  that is the data of second pre/post sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVLwoIfZw72L"
   },
   "source": [
    "## Adjust the model\n",
    "### Model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAcrx-BQE_Yk"
   },
   "outputs": [],
   "source": [
    "input_text = layers.Input(shape=(1,), name='Target_sentences', dtype=tf.string)\n",
    "input_pre = layers.Input(shape=(1,), name='Pre_sentences', dtype=tf.string)\n",
    "input_post = layers.Input(shape=(1,), name='Post_sentences', dtype=tf.string)\n",
    "#%% added\n",
    "#===============================================================================\n",
    "input_prepre = layers.Input(shape=(1,), name='PrePre_sentences', dtype=tf.string)\n",
    "input_postpost = layers.Input(shape=(1,), name='PostPost_sentences', dtype=tf.string)\n",
    "#===============================================================================\n",
    "input_position = layers.Input(shape=(2,), name='Sentence_position', dtype=tf.float32)\n",
    "\n",
    "embedding_target = ElmoEmbeddingLayer(name=\"ElmoEmbed_target\")(input_text)\n",
    "GRU_target = layers.Bidirectional(layers.GRU(256, dropout=0.5, return_sequences=False), name='BiGRU_target')(embedding_target)\n",
    "\n",
    "embedding_pre = ElmoEmbeddingLayer(name=\"ElmoEmbed_pre\")(input_pre)\n",
    "GRU_pre = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_pre')(embedding_pre)\n",
    "embedding_post = ElmoEmbeddingLayer(name=\"ElmoEmbed_post\")(input_post)\n",
    "GRU_post = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_post')(embedding_post)\n",
    "#%% added\n",
    "#===============================================================================\n",
    "embedding_prepre = ElmoEmbeddingLayer(name=\"ElmoEmbed_prepre\")(input_prepre)\n",
    "GRU_prepre = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_prepre')(embedding_prepre)\n",
    "embedding_postpost = ElmoEmbeddingLayer(name=\"ElmoEmbed_postpost\")(input_postpost)\n",
    "GRU_postpost = layers.Bidirectional(layers.GRU(64, dropout=0.5, return_sequences=False), name='BiGRU_postpost')(embedding_postpost)\n",
    "#===============================================================================\n",
    "\n",
    "pre = layers.Dense(20, activation='sigmoid')(GRU_pre)\n",
    "post = layers.Dense(20, activation='sigmoid')(GRU_post)\n",
    "#%% added\n",
    "#===============================================================================\n",
    "prepre = layers.Dense(20, activation='sigmoid')(GRU_prepre)\n",
    "postpost = layers.Dense(20, activation='sigmoid')(GRU_postpost)\n",
    "#===============================================================================\n",
    "\n",
    "#%% add prepre postpost\n",
    "concat = layers.concatenate([prepre, pre, GRU_target, post, postpost, input_position], name='Merge')\n",
    "\n",
    "pred = layers.Dense(6, activation='sigmoid')(concat)\n",
    "#%% add prepre postpost input\n",
    "model = Model(inputs=[input_text,input_pre,input_post,input_position,input_prepre,input_postpost], outputs=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9muRq8kGspA"
   },
   "source": [
    "### Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVbPbYPHGrmC"
   },
   "outputs": [],
   "source": [
    "input_text = layers.Input(shape=(1,), name='Target_sentences', dtype=tf.string)\n",
    "input_pre = layers.Input(shape=(1,), name='Pre_sentences', dtype=tf.string)\n",
    "input_post = layers.Input(shape=(1,), name='Post_sentences', dtype=tf.string)\n",
    "#%% added\n",
    "#===============================================================================\n",
    "input_prepre = layers.Input(shape=(1,), name='PrePre_sentences', dtype=tf.string)\n",
    "input_postpost = layers.Input(shape=(1,), name='PostPost_sentences', dtype=tf.string)\n",
    "#===============================================================================\n",
    "input_position = layers.Input(shape=(2,), name='Sentence_position', dtype=tf.float32)\n",
    "\n",
    "embedding_target = ElmoEmbeddingLayer(name=\"ElmoEmbed_target\")(input_text)\n",
    "GRU_target = layers.Bidirectional(layers.GRU(128, dropout=0.5, return_sequences=False), name='BiGRU_target')(embedding_target)\n",
    "embedding_pre = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_pre\")(input_pre)\n",
    "GRU_pre = layers.Dense(256, activation='sigmoid')(embedding_pre)\n",
    "embedding_post = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_post\")(input_post)\n",
    "GRU_post = layers.Dense(256, activation='sigmoid')(embedding_post)\n",
    "\n",
    "#%% added\n",
    "#===============================================================================\n",
    "embedding_prepre = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_prepre\")(input_prepre)\n",
    "GRU_prepre = layers.Dense(256, activation='sigmoid')(embedding_prepre)\n",
    "embedding_postpost = ElmoSentenceEmbeddingLayer(name=\"ElmoEmbed_postpost\")(input_postpost)\n",
    "GRU_postpost = layers.Dense(256, activation='sigmoid')(embedding_postpost)\n",
    "#===============================================================================\n",
    "\n",
    "#%% add prepre postpost GRU result\n",
    "concat =  layers.Lambda(concatresult, name='Merge_Sentences')([GRU_prepre, GRU_pre, GRU_target, GRU_post, GRU_postpost])\n",
    "Summary = layers.Bidirectional(layers.GRU(128, dropout=0.5, return_sequences=False), name='BiGRU_Summary')(concat)\n",
    "\n",
    "addposition = layers.concatenate([Summary, input_position], name='Merge_position')\n",
    "pred = layers.Dense(6, activation='sigmoid')(addposition)\n",
    "\n",
    "#%% add prepre postpost input\n",
    "model = Model(inputs=[input_text,input_pre,input_post,input_position,input_prepre,input_postpost], outputs=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YUwH1rq2_Hp"
   },
   "source": [
    "## Input becomes\n",
    "```\n",
    "[Sen,PreSen,PostSen,POSITION,PrePreSen,PostPostSen]\n",
    "```\n",
    "\n",
    "**Remember to include the PrePre/PostPost Input !!!!!!!!!**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ELMO_with pre/post sentences.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
